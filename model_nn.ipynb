{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras==2.2.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.5.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.18.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (5.3.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tables\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ed/c3/8fd9e3bb21872f9d69eb93b3014c86479864cca94e625fd03713ccacec80/tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numexpr>=2.6.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/36/ed/eac5f6123f54a61cd13b7e89826b97edea54adf76d9f8e9fa2ce70e2fdf8/numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tables) (1.18.5)\n",
      "Installing collected packages: numexpr, tables\n",
      "Successfully installed numexpr-2.7.1 tables-3.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import *\n",
    "from keras.initializers import RandomNormal,glorot_uniform\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "注释的代码均是预处理代码，先运行一遍，再整体跑\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_data = pd.read_csv('train_data.csv',sep = '|')\n",
    "# test_data_A = pd.read_csv('test_data_A.csv',sep = '|')\n",
    "# test_data_B = pd.read_csv('test_data_B.csv',sep = '|')\n",
    "# test_data_B['pt_d']-=1\n",
    "# test_data = pd.concat([test_data_A,test_data_B]).reset_index(drop=True)\n",
    "# all_data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "\n",
    "# all_col=['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "#        'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "#        'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "#        'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "#        'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "#        'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "#        'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
    "#         'communication_avgonline_30d', 'indu_name']\n",
    "# for i in tqdm(all_col):\n",
    "#     lbl = LabelEncoder()\n",
    "#     all_data[i] = lbl.fit_transform(all_data[i])\n",
    "# train_data = all_data.iloc[0:len(train_data),:]\n",
    "# test_data = all_data.iloc[len(train_data):,:].reset_index(drop=True)\n",
    "# train_data.to_hdf('train_data.h5', key='data')\n",
    "# test_data.to_hdf('test_data.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col=['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "       'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
    "        'communication_avgonline_30d', 'indu_name']#,'expo_day_counts','expo_lastday_counts','click_lastday_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_hdf('train_data.h5', key='data')\n",
    "test_data = pd.read_hdf('test_data.h5', key='data')\n",
    "#all_data = pd.concat([train_data, test_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = train_data[train_data['label']==1].groupby(['uid','pt_d']).agg({'adv_id':list}).reset_index()\n",
    "# temp_data = temp_data.sort_values(['uid','pt_d'])\n",
    "# uid = temp_data['uid'].tolist()\n",
    "# pt_d = temp_data['pt_d'].tolist()\n",
    "# adv_id = temp_data['adv_id'].tolist()\n",
    "# res = []\n",
    "# for i in tqdm(range(0,len(uid))):\n",
    "#     res.append([uid[i],pt_d[i],','.join(list(map(str,adv_id[i])))])\n",
    "# total_days = 8\n",
    "# result = [[res[0][0],res[0][1]+1,res[0][2]]]\n",
    "# for i in tqdm(range(1,len(res))):\n",
    "#     if res[i][0] == result[-1][0]:\n",
    "#         while res[i][1]>result[-1][1]:\n",
    "#             result.append([result[-1][0],result[-1][1]+1,result[-1][2]])\n",
    "#         while res[i][1]>=result[-1][1]:\n",
    "#             result.append([result[-1][0], result[-1][1] + 1, result[-1][2]+','+res[i][2]])\n",
    "#     else:\n",
    "#         while total_days>result[-1][1]:\n",
    "#             result.append([result[-1][0],result[-1][1]+1,result[-1][2]])\n",
    "#         result.append([res[i][0], res[i][1]+1, res[i][2]])\n",
    "# result = pd.DataFrame(result)\n",
    "# result.columns = ['uid','pt_d','his_adv_id']\n",
    "# result['his_adv_id'] = result['his_adv_id'].apply(lambda x:list(map(int,x.split(','))))\n",
    "# result['his_advid_len'] = result['his_adv_id'].apply(lambda x:len(x))\n",
    "# train_data = train_data.merge(result,on = ['uid','pt_d'],how='left')\n",
    "# test_data = test_data.merge(result,on = ['uid','pt_d'],how='left')\n",
    "# train_data = train_data.fillna(0)\n",
    "# test_data = test_data.fillna(0)\n",
    "# his_adv_id = train_data['his_adv_id'].tolist()\n",
    "# for i in tqdm(range(len(his_adv_id))):\n",
    "#     if his_adv_id[i]==0:\n",
    "#         his_adv_id[i] = [0]*20\n",
    "#     else:\n",
    "#         his_adv_id[i] =[0]*(20-len(his_adv_id[i]))+his_adv_id[i][::-1][0:20]\n",
    "# his_adv_id = np.array(his_adv_id)\n",
    "# test_his_adv_id = test_data['his_adv_id'].tolist()\n",
    "# for i in tqdm(range(len(test_his_adv_id))):\n",
    "#     if test_his_adv_id[i]==0:\n",
    "#         test_his_adv_id[i] = [0]*20\n",
    "#     else:\n",
    "#         test_his_adv_id[i] =[0]*(20-len(test_his_adv_id[i]))+test_his_adv_id[i][::-1][0:20]\n",
    "# test_his_adv_id = np.array(test_his_adv_id)\n",
    "# np.save('train_his_advid.npy', his_adv_id)\n",
    "# np.save('test_his_advid.npy', test_his_adv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_his_advid = np.load('train_his_advid.npy')\n",
    "test_his_advid = np.load('test_his_advid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = train_data[train_data['label']==1].groupby(['uid','pt_d']).agg({'adv_prim_id':list}).reset_index()\n",
    "# temp_data = temp_data.sort_values(['uid','pt_d'])\n",
    "# uid = temp_data['uid'].tolist()\n",
    "# pt_d = temp_data['pt_d'].tolist()\n",
    "# adv_id = temp_data['adv_prim_id'].tolist()\n",
    "# res = []\n",
    "# for i in tqdm(range(0,len(uid))):\n",
    "#     res.append([uid[i],pt_d[i],','.join(list(map(str,adv_id[i])))])\n",
    "# total_days = 8\n",
    "# result = [[res[0][0],res[0][1]+1,res[0][2]]]\n",
    "# for i in tqdm(range(1,len(res))):\n",
    "#     if res[i][0] == result[-1][0]:\n",
    "#         while res[i][1]>result[-1][1]:\n",
    "#             result.append([result[-1][0],result[-1][1]+1,result[-1][2]])\n",
    "#         while res[i][1]>=result[-1][1]:\n",
    "#             result.append([result[-1][0], result[-1][1] + 1, result[-1][2]+','+res[i][2]])\n",
    "#     else:\n",
    "#         while total_days>result[-1][1]:\n",
    "#             result.append([result[-1][0],result[-1][1]+1,result[-1][2]])\n",
    "#         result.append([res[i][0], res[i][1]+1, res[i][2]])\n",
    "# result = pd.DataFrame(result)\n",
    "# result.columns = ['uid','pt_d','his_adv_prim_id']\n",
    "# result['his_adv_prim_id'] = result['his_adv_prim_id'].apply(lambda x:list(map(int,x.split(','))))\n",
    "# train_data = train_data.merge(result,on = ['uid','pt_d'],how='left')\n",
    "# test_data = test_data.merge(result,on = ['uid','pt_d'],how='left')\n",
    "# train_data = train_data.fillna(0)\n",
    "# test_data = test_data.fillna(0)\n",
    "# his_adv_id = train_data['his_adv_prim_id'].tolist()\n",
    "# for i in tqdm(range(len(his_adv_id))):\n",
    "#     if his_adv_id[i]==0:\n",
    "#         his_adv_id[i] = [0]*20\n",
    "#     else:\n",
    "#         his_adv_id[i] =[0]*(20-len(his_adv_id[i]))+his_adv_id[i][::-1][0:20]\n",
    "# his_adv_id = np.array(his_adv_id)\n",
    "# test_his_adv_id = test_data['his_adv_prim_id'].tolist()\n",
    "# for i in tqdm(range(len(test_his_adv_id))):\n",
    "#     if test_his_adv_id[i]==0:\n",
    "#         test_his_adv_id[i] = [0]*20\n",
    "#     else:\n",
    "#         test_his_adv_id[i] =[0]*(20-len(test_his_adv_id[i]))+test_his_adv_id[i][::-1][0:20]\n",
    "# test_his_adv_id = np.array(test_his_adv_id)\n",
    "# np.save('train_his_adv_prim_id.npy', his_adv_id)\n",
    "# np.save('test_his_adv_prim_id.npy', test_his_adv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_his_adv_prim_id = np.load('train_his_adv_prim_id.npy')\n",
    "test_his_adv_prim_id = np.load('test_his_adv_prim_id.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "# params = {'feature_config':{}}\n",
    "# for i in tqdm(all_col):\n",
    "#     params['feature_config'][i] = {}\n",
    "#     params['feature_config'][i]['vocabulary_size'] = all_data[i].max()+1\n",
    "# #     if i in {'task_id','adv_id'}:\n",
    "# #         params['feature_config'][i]['embedding_dim'] = 32\n",
    "# #     else:\n",
    "#     params['feature_config'][i]['embedding_dim'] = 16\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'feature_config': {'task_id': {'vocabulary_size': 4932, 'embedding_dim': 16},\n",
    "  'adv_id': {'vocabulary_size': 5956, 'embedding_dim': 16},\n",
    "  'creat_type_cd': {'vocabulary_size': 8, 'embedding_dim': 16},\n",
    "  'adv_prim_id': {'vocabulary_size': 114, 'embedding_dim': 16},\n",
    "  'dev_id': {'vocabulary_size': 61, 'embedding_dim': 16},\n",
    "  'inter_type_cd': {'vocabulary_size': 4, 'embedding_dim': 16},\n",
    "  'slot_id': {'vocabulary_size': 12, 'embedding_dim': 16},\n",
    "  'spread_app_id': {'vocabulary_size': 79, 'embedding_dim': 16},\n",
    "  'tags': {'vocabulary_size': 32, 'embedding_dim': 16},\n",
    "  'app_first_class': {'vocabulary_size': 3, 'embedding_dim': 16},\n",
    "  'app_second_class': {'vocabulary_size': 19, 'embedding_dim': 16},\n",
    "  'age': {'vocabulary_size': 8, 'embedding_dim': 16},\n",
    "  'city': {'vocabulary_size': 344, 'embedding_dim': 16},\n",
    "  'city_rank': {'vocabulary_size': 4, 'embedding_dim': 16},\n",
    "  'device_name': {'vocabulary_size': 94, 'embedding_dim': 16},\n",
    "  'device_size': {'vocabulary_size': 231, 'embedding_dim': 16},\n",
    "  'career': {'vocabulary_size': 9, 'embedding_dim': 16},\n",
    "  'gender': {'vocabulary_size': 3, 'embedding_dim': 16},\n",
    "  'net_type': {'vocabulary_size': 5, 'embedding_dim': 16},\n",
    "  'residence': {'vocabulary_size': 36, 'embedding_dim': 16},\n",
    "  'his_app_size': {'vocabulary_size': 21, 'embedding_dim': 16},\n",
    "  'his_on_shelf_time': {'vocabulary_size': 4, 'embedding_dim': 16},\n",
    "  'app_score': {'vocabulary_size': 2, 'embedding_dim': 16},\n",
    "  'emui_dev': {'vocabulary_size': 18, 'embedding_dim': 16},\n",
    "  'list_time': {'vocabulary_size': 19, 'embedding_dim': 16},\n",
    "  'device_price': {'vocabulary_size': 8, 'embedding_dim': 16},\n",
    "  'up_life_duration': {'vocabulary_size': 21, 'embedding_dim': 16},\n",
    "  'up_membership_grade': {'vocabulary_size': 4, 'embedding_dim': 16},\n",
    "  'membership_life_duration': {'vocabulary_size': 21, 'embedding_dim': 16},\n",
    "  'consume_purchase': {'vocabulary_size': 9, 'embedding_dim': 16},\n",
    "  'communication_avgonline_30d': {'vocabulary_size': 14, 'embedding_dim': 16},\n",
    "  'indu_name': {'vocabulary_size': 42, 'embedding_dim': 16}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CIN(Layer):\n",
    "#     def __init__(self, layer_size=(128, 128), activation='relu', split_half=True, l2_reg=0, seed=1024, **kwargs):\n",
    "#         self.layer_size = layer_size\n",
    "#         self.split_half = split_half\n",
    "#         self.activation = activation\n",
    "#         self.l2_reg = l2_reg\n",
    "#         self.seed = seed\n",
    "#         super(CIN, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.field_nums = [int(input_shape[1])]\n",
    "#         self.filters = []\n",
    "#         self.bias = []\n",
    "#         for i, size in enumerate(self.layer_size):\n",
    "\n",
    "#             self.filters.append(self.add_weight(name='filter' + str(i),\n",
    "#                                                 shape=[1, self.field_nums[-1]\n",
    "#                                                        * self.field_nums[0], size],\n",
    "#                                                 dtype=tf.float32, initializer=glorot_uniform(\n",
    "#                     seed=self.seed + i)))\n",
    "\n",
    "#             self.bias.append(self.add_weight(name='bias' + str(i), shape=[size], dtype=tf.float32,\n",
    "#                                              initializer=tf.keras.initializers.Zeros()))\n",
    "\n",
    "#             if self.split_half:\n",
    "#                 if i != len(self.layer_size) - 1 and size % 2 > 0:\n",
    "#                     raise ValueError(\n",
    "#                         \"layer_size must be even number except for the last layer when split_half=True\")\n",
    "#                 self.field_nums.append(size // 2)\n",
    "#             else:\n",
    "#                 self.field_nums.append(size)\n",
    "\n",
    "#         self.activation_layers = [Activation(self.activation) for _ in self.layer_size]\n",
    "\n",
    "#         super(CIN, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         dim = int(inputs.get_shape()[-1])\n",
    "#         hidden_nn_layers = [inputs]\n",
    "#         final_result = []\n",
    "\n",
    "#         split_tensor0 = tf.split(hidden_nn_layers[0], dim * [1], 2)\n",
    "#         for idx, layer_size in enumerate(self.layer_size):\n",
    "#             split_tensor = tf.split(hidden_nn_layers[-1], dim * [1], 2)\n",
    "\n",
    "#             dot_result_m = tf.matmul(\n",
    "#                 split_tensor0, split_tensor, transpose_b=True)\n",
    "\n",
    "#             dot_result_o = tf.reshape(\n",
    "#                 dot_result_m, shape=[dim, -1, self.field_nums[0] * self.field_nums[idx]])\n",
    "\n",
    "#             dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])\n",
    "\n",
    "#             curr_out = tf.nn.conv1d(\n",
    "#                 dot_result, filters=self.filters[idx], stride=1, padding='VALID')\n",
    "\n",
    "#             curr_out = tf.nn.bias_add(curr_out, self.bias[idx])\n",
    "\n",
    "#             curr_out = self.activation_layers[idx](curr_out)\n",
    "\n",
    "#             curr_out = tf.transpose(curr_out, perm=[0, 2, 1])\n",
    "\n",
    "#             if self.split_half:\n",
    "#                 if idx != len(self.layer_size) - 1:\n",
    "#                     next_hidden, direct_connect = tf.split(\n",
    "#                         curr_out, 2 * [layer_size // 2], 1)\n",
    "#                 else:\n",
    "#                     direct_connect = curr_out\n",
    "#                     next_hidden = 0\n",
    "#             else:\n",
    "#                 direct_connect = curr_out\n",
    "#                 next_hidden = curr_out\n",
    "\n",
    "#             final_result.append(direct_connect)\n",
    "#             hidden_nn_layers.append(next_hidden)\n",
    "\n",
    "#         result = tf.concat(final_result, axis=1)\n",
    "#         result = tf.reduce_sum(result, -1, keep_dims=False)\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         if self.split_half:\n",
    "#             featuremap_num = sum(\n",
    "#                 self.layer_size[:-1]) // 2 + self.layer_size[-1]\n",
    "#         else:\n",
    "#             featuremap_num = sum(self.layer_size)\n",
    "#         return (None, featuremap_num)\n",
    "\n",
    "#     def get_config(self, ):\n",
    "\n",
    "#         config = {'layer_size': self.layer_size, 'split_half': self.split_half, 'activation': self.activation,\n",
    "#                   'seed': self.seed}\n",
    "#         base_config = super(CIN, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class FM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super(FM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(FM, self).build(input_shape) \n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \n",
    "        ad_group = tf.reduce_sum(inputs[0], axis=1)\n",
    "        user_group = tf.reduce_sum(inputs[1], axis=1)\n",
    "        cross_term = tf.reduce_sum(ad_group*user_group,axis=1, keep_dims=True)\n",
    "        return cross_term\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "class Add(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Add, self).__init__(**kwargs)\n",
    "    def call(self, input, **kwargs):\n",
    "        return input[0]+input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self,params):\n",
    "        '''\n",
    "        features : 特征名\n",
    "        '''\n",
    "        self.params = params\n",
    "        self.input_dict,self.embedding_dict,self.embedding_lookup = self.build_input(self.params['feature_config'])\n",
    "        self.model = self.build_model(self.input_dict,self.embedding_lookup,self.embedding_dict)\n",
    "   #     print(self.model.summary())\n",
    "        \n",
    "    def build_input(self,features):\n",
    "        input_dict = {}\n",
    "        embedding_dict = {}\n",
    "        embedding_lookup = {}\n",
    "        for fea in features:\n",
    "            feature_embedding = Embedding(\n",
    "                features[fea]['vocabulary_size'],features[fea]['embedding_dim'],\n",
    "                embeddings_initializer=RandomNormal(mean=0.0, stddev=0.0001, seed=1996),\n",
    "                name=\"emb_\" + fea)\n",
    "            embedding_dict[fea] = feature_embedding\n",
    "            fea_input = Input(shape=(1,),name = fea)\n",
    "            input_dict[fea] = fea_input\n",
    "            embedding = feature_embedding(fea_input)\n",
    "            embedding_lookup[fea] = embedding\n",
    "        input_dict['his_adv_id'] = Input(shape=(20,),name = 'his_adv_id')\n",
    "        input_dict['his_adv_prim_id'] = Input(shape=(20,),name = 'his_adv_prim_id')\n",
    "      #  input_dict['advid_ld'] = Input(shape=(20,),name = 'advid_ld')\n",
    "        \n",
    "        return input_dict,embedding_dict,embedding_lookup\n",
    "    '''\n",
    "    ,'slot_id'\n",
    "    '''\n",
    "    def build_model(self,input_dict,embedding_lookup,embedding_dict):\n",
    "       # FMinput = concatenate([embedding_lookup[i] for i in embedding_lookup],axis = 1)\n",
    "       # exFM_out = CIN()(FMinput)\n",
    "       # exFM_logit = Dense(1, activation=None, )(exFM_out)\n",
    "        \n",
    "        ad_group = concatenate([embedding_lookup[i] for i in ['adv_id','task_id','creat_type_cd','adv_prim_id','dev_id',\n",
    "                            'inter_type_cd','spread_app_id','tags','app_first_class','app_second_class','his_app_size','his_on_shelf_time',\n",
    "                                                             'app_score','indu_name']],axis = 1)\n",
    "        \n",
    "        user_group = concatenate([embedding_lookup[i] for i in ['age','city','city_rank','device_name','device_size',\n",
    "                            'career','gender','net_type','residence','emui_dev','list_time','device_price','up_life_duration','up_membership_grade',\n",
    "                                        'membership_life_duration','consume_purchase','communication_avgonline_30d']],axis = 1)\n",
    "        \n",
    "        \n",
    "        FMout = FM(name = 'FM')([ad_group,user_group])\n",
    "        his_adv_id = embedding_dict['adv_id'](input_dict['his_adv_id'])\n",
    "        his_adv_id = Bidirectional(CuDNNGRU(128))(his_adv_id)\n",
    "        his_adv_prim_id = embedding_dict['adv_prim_id'](input_dict['his_adv_prim_id'])\n",
    "        his_adv_prim_id = Bidirectional(CuDNNGRU(128))(his_adv_prim_id)\n",
    "     #   user_app_rep = embedding_dict['adv_id'](input_dict['advid_ld'])\n",
    "     #   user_app_rep = Bidirectional(CuDNNGRU(128))(user_app_rep)\n",
    "     #   his_adv_id0 = embedding_dict['adv_id'](input_dict['his_adv_id0'])\n",
    "     #   his_adv_id0 = Bidirectional(CuDNNGRU(128))(his_adv_id0)\n",
    "        embedding_concat = Flatten()(concatenate([embedding_lookup[i] for i in embedding_lookup]))\n",
    "        embedding_concat=concatenate([embedding_concat,his_adv_id,his_adv_prim_id,])#user_app_rep\n",
    "      #  linear = Dense(1)(his_adv_id0)\n",
    "        dense = Dropout(0.3)(embedding_concat)\n",
    "        dense = Dense(1024)(dense)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = Activation(activation=\"relu\")(dense)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        dense = Dense(512)(dense)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = Activation(activation=\"relu\")(dense)\n",
    "        dense = Dense(1)(dense)\n",
    "        FMout = Dense(1)(FMout)\n",
    "        output = Add()([dense,FMout])#exFM_logit\n",
    "        output = Activation(activation=\"sigmoid\")(output)\n",
    "        model = Model(inputs=[input_dict[i] for i in input_dict], outputs=output)\n",
    "        model = multi_gpu_model(model, gpus=4)\n",
    "        model.compile(optimizer ='adam',\n",
    "                      loss= 'binary_crossentropy',\n",
    "                      metrics=['acc'])\n",
    "        return model\n",
    "        \n",
    "    def train(self,train_data,valid_data,train_label,valid_label,batch_size,epochs,callbacks = None):\n",
    "        self.model.fit(train_data,train_label, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "                  validation_data=(valid_data, valid_label),callbacks=callbacks)\n",
    "\n",
    "    def predict(self,test_data,batch_size=4096):\n",
    "        result = self.model.predict(test_data, batch_size=batch_size)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(ind):\n",
    "    data=dict()\n",
    "    for i in params['feature_config']:\n",
    "        data[i] = np.array(train_data[i][ind])\n",
    "    data['his_adv_id'] = train_his_advid[ind,:]\n",
    "    data['his_adv_prim_id'] = train_his_adv_prim_id[ind,:]\n",
    "   # data['advid_ld'] = train_advid_ld[ind,:]\n",
    "    return data\n",
    "def make_train():\n",
    "    data=dict()\n",
    "    for i in params['feature_config']:\n",
    "        data[i] = np.array(train_data[i][0:35897957])\n",
    "    data['his_adv_id'] = train_his_advid[0:35897957,:]\n",
    "    data['his_adv_prim_id'] = train_his_adv_prim_id[0:35897957,:]\n",
    "  #  data['his_adv_id0'] = train_his_advid0[0:35897957,:]\n",
    "    return data\n",
    "def make_valid():\n",
    "    data=dict()\n",
    "    for i in params['feature_config']:\n",
    "        data[i] = np.array(train_data[i][35897957:])\n",
    "    data['his_adv_id'] = train_his_advid[35897957:,:]\n",
    "    data['his_adv_prim_id'] = train_his_adv_prim_id[35897957:,:]\n",
    " #   data['his_adv_id0'] = train_his_advid0[35897957:,:]\n",
    "    return data\n",
    "def make_test():\n",
    "    data=dict()\n",
    "    for i in params['feature_config']:\n",
    "        data[i] = np.array(test_data[i])\n",
    "    data['his_adv_id'] = test_his_advid\n",
    "    data['his_adv_prim_id'] = test_his_adv_prim_id\n",
    "   # data['advid_ld'] = test_advid_ld\n",
    " #   data['his_task_id'] = test_his_task_id\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-13-2b9312148968>:110: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 33525706 samples, validate on 8381427 samples\n",
      "Epoch 1/10\n",
      "33525706/33525706 [==============================] - 678s 20us/step - loss: 0.1348 - acc: 0.9653 - val_loss: 0.1331 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13312, saving model to ffm_model0.h5\n",
      "Epoch 2/10\n",
      "33525706/33525706 [==============================] - 660s 20us/step - loss: 0.1328 - acc: 0.9655 - val_loss: 0.1325 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13312 to 0.13252, saving model to ffm_model0.h5\n",
      "Epoch 3/10\n",
      "33525706/33525706 [==============================] - 661s 20us/step - loss: 0.1323 - acc: 0.9655 - val_loss: 0.1324 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13252 to 0.13237, saving model to ffm_model0.h5\n",
      "Epoch 4/10\n",
      "33525706/33525706 [==============================] - 660s 20us/step - loss: 0.1319 - acc: 0.9655 - val_loss: 0.1323 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13237 to 0.13231, saving model to ffm_model0.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/10\n",
      "33525706/33525706 [==============================] - 661s 20us/step - loss: 0.1316 - acc: 0.9655 - val_loss: 0.1322 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13231 to 0.13222, saving model to ffm_model0.h5\n",
      "Epoch 6/10\n",
      "33525706/33525706 [==============================] - 660s 20us/step - loss: 0.1314 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13222 to 0.13214, saving model to ffm_model0.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 7/10\n",
      "33525706/33525706 [==============================] - 660s 20us/step - loss: 0.1312 - acc: 0.9655 - val_loss: 0.1322 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13214\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00007: early stopping\n",
      "Train on 33525706 samples, validate on 8381427 samples\n",
      "Epoch 1/10\n",
      "33525706/33525706 [==============================] - 665s 20us/step - loss: 0.1346 - acc: 0.9654 - val_loss: 0.1330 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13302, saving model to ffm_model1.h5\n",
      "Epoch 2/10\n",
      "33525706/33525706 [==============================] - 657s 20us/step - loss: 0.1328 - acc: 0.9655 - val_loss: 0.1325 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13302 to 0.13251, saving model to ffm_model1.h5\n",
      "Epoch 3/10\n",
      "33525706/33525706 [==============================] - 658s 20us/step - loss: 0.1322 - acc: 0.9655 - val_loss: 0.1323 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13251 to 0.13233, saving model to ffm_model1.h5\n",
      "Epoch 4/10\n",
      "33525706/33525706 [==============================] - 658s 20us/step - loss: 0.1319 - acc: 0.9655 - val_loss: 0.1323 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13233 to 0.13227, saving model to ffm_model1.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/10\n",
      "33525706/33525706 [==============================] - 664s 20us/step - loss: 0.1316 - acc: 0.9655 - val_loss: 0.1322 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13227 to 0.13216, saving model to ffm_model1.h5\n",
      "Epoch 6/10\n",
      "33525706/33525706 [==============================] - 663s 20us/step - loss: 0.1314 - acc: 0.9655 - val_loss: 0.1323 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13216\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 7/10\n",
      "33525706/33525706 [==============================] - 658s 20us/step - loss: 0.1312 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13216 to 0.13213, saving model to ffm_model1.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00007: early stopping\n",
      "Train on 33525706 samples, validate on 8381427 samples\n",
      "Epoch 1/10\n",
      "33525706/33525706 [==============================] - 682s 20us/step - loss: 0.1346 - acc: 0.9654 - val_loss: 0.1330 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13301, saving model to ffm_model2.h5\n",
      "Epoch 2/10\n",
      "33525706/33525706 [==============================] - 680s 20us/step - loss: 0.1328 - acc: 0.9655 - val_loss: 0.1324 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13301 to 0.13236, saving model to ffm_model2.h5\n",
      "Epoch 3/10\n",
      "33525706/33525706 [==============================] - 656s 20us/step - loss: 0.1323 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13236 to 0.13213, saving model to ffm_model2.h5\n",
      "Epoch 4/10\n",
      "33525706/33525706 [==============================] - 655s 20us/step - loss: 0.1320 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13213 to 0.13207, saving model to ffm_model2.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/10\n",
      "33525706/33525706 [==============================] - 657s 20us/step - loss: 0.1317 - acc: 0.9655 - val_loss: 0.1319 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13207 to 0.13194, saving model to ffm_model2.h5\n",
      "Epoch 6/10\n",
      "33525706/33525706 [==============================] - 673s 20us/step - loss: 0.1315 - acc: 0.9655 - val_loss: 0.1319 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13194 to 0.13192, saving model to ffm_model2.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 7/10\n",
      "33525706/33525706 [==============================] - 662s 20us/step - loss: 0.1313 - acc: 0.9655 - val_loss: 0.1320 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13192\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00007: early stopping\n",
      "Train on 33525707 samples, validate on 8381426 samples\n",
      "Epoch 1/10\n",
      "33525707/33525707 [==============================] - 666s 20us/step - loss: 0.1346 - acc: 0.9653 - val_loss: 0.1330 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13300, saving model to ffm_model3.h5\n",
      "Epoch 2/10\n",
      "33525707/33525707 [==============================] - 657s 20us/step - loss: 0.1328 - acc: 0.9655 - val_loss: 0.1327 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13300 to 0.13270, saving model to ffm_model3.h5\n",
      "Epoch 3/10\n",
      "33525707/33525707 [==============================] - 658s 20us/step - loss: 0.1322 - acc: 0.9655 - val_loss: 0.1323 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13270 to 0.13227, saving model to ffm_model3.h5\n",
      "Epoch 4/10\n",
      "33525707/33525707 [==============================] - 658s 20us/step - loss: 0.1319 - acc: 0.9655 - val_loss: 0.1322 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13227 to 0.13223, saving model to ffm_model3.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 5/10\n",
      "33525707/33525707 [==============================] - 658s 20us/step - loss: 0.1316 - acc: 0.9655 - val_loss: 0.1322 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13223 to 0.13216, saving model to ffm_model3.h5\n",
      "Epoch 6/10\n",
      "33525707/33525707 [==============================] - 659s 20us/step - loss: 0.1314 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13216 to 0.13214, saving model to ffm_model3.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 7/10\n",
      "33525707/33525707 [==============================] - 659s 20us/step - loss: 0.1312 - acc: 0.9655 - val_loss: 0.1323 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13214\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00007: early stopping\n",
      "Train on 33525707 samples, validate on 8381426 samples\n",
      "Epoch 1/10\n",
      "33525707/33525707 [==============================] - 662s 20us/step - loss: 0.1346 - acc: 0.9654 - val_loss: 0.1329 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13290, saving model to ffm_model4.h5\n",
      "Epoch 2/10\n",
      "33525707/33525707 [==============================] - 652s 19us/step - loss: 0.1328 - acc: 0.9655 - val_loss: 0.1324 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13290 to 0.13239, saving model to ffm_model4.h5\n",
      "Epoch 3/10\n",
      "33525707/33525707 [==============================] - 652s 19us/step - loss: 0.1322 - acc: 0.9655 - val_loss: 0.1324 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13239\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 4/10\n",
      "33525707/33525707 [==============================] - 654s 20us/step - loss: 0.1318 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13239 to 0.13213, saving model to ffm_model4.h5\n",
      "Epoch 5/10\n",
      "33525707/33525707 [==============================] - 653s 19us/step - loss: 0.1317 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13213 to 0.13210, saving model to ffm_model4.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 6/10\n",
      "33525707/33525707 [==============================] - 654s 19us/step - loss: 0.1314 - acc: 0.9655 - val_loss: 0.1321 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13210\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "target = train_data['label']\n",
    "mode = 1\n",
    "stack_test = np.zeros((len(test_data),1))\n",
    "stack_train = np.zeros((train_data.shape[0], 1))\n",
    "if mode == 1:\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=1996, shuffle=True)\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(train_data, target)):\n",
    "        K.clear_session()\n",
    "        filepath = \"ffm_model%d.h5\" % index\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001, verbose=1)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2,verbose=1, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        nn_model = model(params)\n",
    "        trian_x = make_data(train_index)\n",
    "        valid_x = make_data(test_index)\n",
    "        trian_y = np.array(target[train_index])\n",
    "        valid_y = np.array(target[test_index])\n",
    "        nn_model.train(trian_x,valid_x,trian_y,valid_y,4096,10,callbacks=callbacks,)\n",
    "        nn_model.model.load_weights(filepath)\n",
    "        stack_test += nn_model.predict(make_test())\n",
    "        stack_train[test_index] = nn_model.predict(valid_x)\n",
    "        \n",
    "        if index==0:\n",
    "            \n",
    "            reslgb = pd.read_csv('lgbres.csv')\n",
    "            reslgb['probability']=stack_test[1000000:]\n",
    "            reslgb[['id','probability']].to_csv('subnn.csv',index=False)\n",
    "else:\n",
    "    trian_x = make_train()\n",
    "    valid_x = make_valid()\n",
    "    trian_y = np.array(target[0:35897957])\n",
    "    valid_y = np.array(target[35897957:])\n",
    "    model.train(trian_x,valid_x,trian_y,valid_y,4096,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = pd.DataFrame(stack_train)\n",
    "oof_test = pd.DataFrame(stack_test[1000000:]/5)\n",
    "pd.concat([oof_train,oof_test]).to_hdf('nn1oof.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_auc: 0.7651491221203367\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('val_auc:',metrics.roc_auc_score(valid_y,valid_prob))#0.42480\n",
    "#val_auc: 0.7650400675874388  0.776936\n",
    "#val_auc: 0.765866672319938  0.779859\n",
    "#val_loss: 0.1322 val_auc: 0.7660568973066079  0.780272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "val_auc: 0.8127748523069542   \n",
    "val_auc: 0.788409133403723   0.780272\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
